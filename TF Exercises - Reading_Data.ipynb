{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fionachow/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "author = \"kyubyong. https://github.com/Kyubyong/tensorflow-exercises\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE on notation\n",
    "\n",
    "    _x, _y, _z, _X, _Y, _Z, ...: NumPy arrays\n",
    "    x, y, z, X, Y, Z, ...: Tensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data and save to npz.\n",
    "_x = np.zeros((100, 10), np.int32)\n",
    "for i in range(100):\n",
    "    _x[i] = np.random.permutation(10)\n",
    "_x, _y = _x[:, :-1], _x[:, -1]\n",
    "\n",
    "import os\n",
    "if not os.path.exists('example'): os.mkdir('example')\n",
    "np.savez('example/example.npz', _x=_x, _y=_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat = [5 0 6 0 7 6 8 0 7 4 1 6 0 4 1 4 4 0 7 2 7 8 4 3 9 6 0 5 5 3]\n",
      "true y = [5 0 6 0 7 6 8 0 7 4 1 6 0 4 1 4 4 0 7 2 7 8 4 3 9 6 0 5 5 3]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = np.load('example/example.npz')\n",
    "_x, _y = data[\"_x\"], data[\"_y\"]\n",
    "\n",
    "#Q1. Make a placeholder for x such that it should be of dtype=int32, shape=(None, 9).\n",
    "# Inputs and targets\n",
    "x_pl = tf.placeholder(dtype=np.int32, shape=(None, 9))\n",
    "y_hat = 45 - tf.reduce_sum(x_pl, axis=1) # We find a digit x_pl doesn't contain.\n",
    "\n",
    "# Session\n",
    "with tf.Session() as sess:\n",
    "    _y_hat = sess.run(y_hat, {x_pl: _x})\n",
    "    print(\"y_hat =\", _y_hat[:30])\n",
    "    print(\"true y =\", _y[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 == 5; 0 == 0; 6 == 6; 0 == 0; 7 == 7; 6 == 6; 8 == 8; 0 == 0; 7 == 7; 4 == 4; 1 == 1; 6 == 6; 0 == 0; 4 == 4; 1 == 1; 4 == 4; 4 == 4; 0 == 0; 7 == 7; 2 == 2; 7 == 7; 8 == 8; 4 == 4; 3 == 3; 9 == 9; 6 == 6; 0 == 0; 5 == 5; 5 == 5; 3 == 3; 8 == 8; 1 == 1; 8 == 8; 2 == 2; 8 == 8; 5 == 5; 4 == 4; 9 == 9; 2 == 2; 1 == 1; 1 == 1; 2 == 2; 7 == 7; 9 == 9; 8 == 8; 2 == 2; 7 == 7; 3 == 3; 1 == 1; 0 == 0; 8 == 8; 6 == 6; 2 == 2; 1 == 1; 5 == 5; 1 == 1; 3 == 3; 7 == 7; 7 == 7; 9 == 9; 8 == 8; 1 == 1; 0 == 0; 1 == 1; 4 == 4; 7 == 7; 3 == 3; 8 == 8; 5 == 5; 6 == 6; 3 == 3; 5 == 5; 3 == 3; 8 == 8; 4 == 4; 9 == 9; 0 == 0; 3 == 3; 5 == 5; 8 == 8; 6 == 6; 4 == 4; 1 == 1; 6 == 6; 5 == 5; 5 == 5; 9 == 9; 5 == 5; 8 == 8; 6 == 6; 4 == 4; 2 == 2; 9 == 9; 7 == 7; 2 == 2; 9 == 9; 8 == 8; 2 == 2; 7 == 7; 9 == 9; Done training -- epoch limit reached\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Load data\n",
    "data = np.load('example/example.npz')\n",
    "_x, _y = data[\"_x\"], data[\"_y\"]\n",
    "\n",
    "# Serialize\n",
    "with tf.python_io.TFRecordWriter(\"example/tfrecord\") as fout:\n",
    "    for _xx, _yy in zip(_x, _y):\n",
    "        ex = tf.train.Example()\n",
    "        \n",
    "        # Q2. Add each value to ex.\n",
    "        \"\"\"\n",
    "        feature {\n",
    "          key: \"x\"\n",
    "          value {\n",
    "            int64_list {\n",
    "              value: 5\n",
    "              value: 4\n",
    "              value: 6\n",
    "              value: 2\n",
    "              value: 0\n",
    "              value: 1\n",
    "              value: 7\n",
    "              value: 3\n",
    "              value: 8\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "        feature {\n",
    "          key: \"y\"\n",
    "          value {\n",
    "            int64_list {\n",
    "              value: 9\n",
    "            }\n",
    "          }\n",
    "        \"\"\"\n",
    "        ex.features.feature['x'].int64_list.value.extend(_xx)\n",
    "        ex.features.feature['y'].int64_list.value.append(_yy)\n",
    "        fout.write(ex.SerializeToString())\n",
    "\n",
    "def read_and_decode_single_example(fname):\n",
    "    # Create a string queue\n",
    "    \"\"\"<tensorflow.python.ops.data_flow_ops.FIFOQueue object at 0x1815c1feb8>\"\"\"\n",
    "    fname_q = tf.train.string_input_producer([fname], num_epochs=1, shuffle=True)\n",
    "    # Q3. Create a TFRecordReader\n",
    "    reader = tf.TFRecordReader()\n",
    "    \n",
    "    # Read the string queue\n",
    "    _, serialized_example = reader.read(fname_q)\n",
    "    # Q4. Describe parsing syntax\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\"x\":tf.FixedLenFeature([9], tf.int64),\n",
    "                  \"y\":tf.FixedLenFeature([1], tf.int64)}\n",
    "        )\n",
    "    # Output\n",
    "    \"\"\"Tensor(\"ParseSingleExample/ParseSingleExample:0\", shape=(9,), dtype=int64)\"\"\"\n",
    "    x = features['x']\n",
    "    \"\"\"Tensor(\"ParseSingleExample/ParseSingleExample:1\", shape=(1,), dtype=int64)\"\"\"\n",
    "    y = features['y']\n",
    "    return x, y\n",
    "\n",
    "# Ops\n",
    "\"\"\"Tensor(\"ParseSingleExample/ParseSingleExample:1\", shape=(1,), dtype=int64)\"\"\"\n",
    "x, y = read_and_decode_single_example('example/tfrecord')\n",
    "\"\"\"Tensor(\"sub:0\", shape=(), dtype=int64)\"\"\"\n",
    "y_hat = 45 - tf.reduce_sum(x)\n",
    "\n",
    "# Session\n",
    "with tf.Session() as sess:\n",
    "    #Q5. Initialize local variables\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    try:\n",
    "        while not coord.should_stop():\n",
    "            _y, _y_hat = sess.run([y, y_hat])\n",
    "            print(_y[0],\"==\", _y_hat, end=\"; \")\n",
    "    \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done training -- epoch limit reached')\n",
    "    finally:\n",
    "        # When done, ask the threads to stop.\n",
    "        coord.request_stop()\n",
    "    \n",
    "    # Wait for threads to finish.\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 4 2 0 3 6 6 1 0 7] == [5 4 2 0 3 6 6 1 0 7]\n",
      "[6 0 0 9 9 2 7 3 7 5] == [6 0 0 9 9 2 7 3 7 5]\n",
      "[8 9 7 9 4 4 1 3 8 1] == [8 9 7 9 4 4 1 3 8 1]\n",
      "[8 4 7 8 2 7 8 9 4 0] == [8 4 7 8 2 7 8 9 4 0]\n",
      "[7 8 2 8 5 2 9 0 1 7] == [7 8 2 8 5 2 9 0 1 7]\n",
      "[1 8 1 6 4 5 5 6 5 0] == [1 8 1 6 4 5 5 6 5 0]\n",
      "[9 4 1 3 7 6 2 9 5 8] == [9 4 1 3 7 6 2 9 5 8]\n",
      "[5 5 3 3 6 3 1 5 5 0] == [5 5 3 3 6 3 1 5 5 0]\n",
      "[1 8 6 4 7 4 0 8 1 9] == [1 8 6 4 7 4 0 8 1 9]\n",
      "[2 1 8 8 6 2 4 7 3 2] == [2 1 8 8 6 2 4 7 3 2]\n",
      "[9 9 1 5 2 1 1 7 4 1] == [9 9 1 5 2 1 1 7 4 1]\n",
      "[6 7 3 8 6 3 1 5 3 8] == [6 7 3 8 6 3 1 5 3 8]\n",
      "[8 8 8 5 4 6 1 3 7 5] == [8 8 8 5 4 6 1 3 7 5]\n",
      "[2 7 1 4 6 7 4 7 6 7] == [2 7 1 4 6 7 4 7 6 7]\n",
      "[0 1 9 2 5 7 9 4 3 6] == [0 1 9 2 5 7 9 4 3 6]\n",
      "[0 0 2 0 9 0 2 5 5 5] == [0 0 2 0 9 0 2 5 5 5]\n",
      "[8 8 6 9 9 2 6 8 7 3] == [8 8 6 9 9 2 6 8 7 3]\n",
      "[5 8 0 2 4 2 9 8 1 0] == [5 8 0 2 4 2 9 8 1 0]\n",
      "[0 4 5 8 5 8 4 0 9 4] == [0 4 5 8 5 8 4 0 9 4]\n",
      "[3 7 4 7 8 1 1 6 2 3] == [3 7 4 7 8 1 1 6 2 3]\n",
      "Done training -- epoch limit reached\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Load data\n",
    "data = np.load('example/example.npz')\n",
    "_x, _y = data[\"_x\"], data[\"_y\"]\n",
    "\n",
    "# Hyperparams\n",
    "batch_size = 10 # We will feed mini-batches of size 10.\n",
    "num_epochs = 2 # We will feed data for two epochs.\n",
    "\n",
    "# Convert to tensors\n",
    "x = tf.convert_to_tensor(_x)\n",
    "y = tf.convert_to_tensor(_y)\n",
    "\n",
    "# Q6. Make slice queues\n",
    "x_q, y_q = tf.train.slice_input_producer(\n",
    "    tensor_list=[x, y],\n",
    "    num_epochs=num_epochs)\n",
    "\n",
    "# Batching\n",
    "x_batch, y_batch = tf.train.batch([x_q, y_q], batch_size=batch_size)\n",
    "\n",
    "# Targets\n",
    "y_hat = 45 - tf.reduce_sum(x_batch, axis=1)\n",
    "# y_hat, y_batch -> shape(10,)\n",
    "# Session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    # Q7. Make a train.Coordinator and threads.\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    try:\n",
    "        while not coord.should_stop():\n",
    "            _y_hat, _y_batch = sess.run([y_hat, y_batch])\n",
    "            print(_y_hat, \"==\", _y_batch)\n",
    "    \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done training -- epoch limit reached')\n",
    "    finally:\n",
    "        # When done, ask the threads to stop.\n",
    "        coord.request_stop()\n",
    "    \n",
    "    # Wait for threads to finish.\n",
    "    coord.join(threads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Read csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 8 5 5 7 9 7 0 3 6] == [9 8 5 5 7 9 7 0 3 6]\n",
      "[0 1 1 3 1 2 6 8 6 7] == [0 1 1 3 1 2 6 8 6 7]\n",
      "[1 9 4 1 8 0 4 8 9 6] == [1 9 4 1 8 0 4 8 9 6]\n",
      "[0 6 4 2 7 5 1 0 8 6] == [0 6 4 2 7 5 1 0 8 6]\n",
      "[7 4 0 5 9 1 9 9 4 5] == [7 4 0 5 9 1 9 9 4 5]\n",
      "[5 5 5 2 7 4 5 7 8 1] == [5 5 5 2 7 4 5 7 8 1]\n",
      "[5 3 8 3 4 2 0 8 9 0] == [5 3 8 3 4 2 0 8 9 0]\n",
      "[7 8 1 5 5 6 9 8 4 2] == [7 8 1 5 5 6 9 8 4 2]\n",
      "[7 3 8 1 8 6 5 3 1 3] == [7 3 8 1 8 6 5 3 1 3]\n",
      "[1 6 5 5 6 9 7 6 3 6] == [1 6 5 5 6 9 7 6 3 6]\n",
      "[8 9 5 8 7 4 5 3 7 0] == [8 9 5 8 7 4 5 3 7 0]\n",
      "[7 7 3 1 0 3 1 7 6 6] == [7 7 3 1 0 3 1 7 6 6]\n",
      "[0 2 4 9 9 1 3 9 0 1] == [0 2 4 9 9 1 3 9 0 1]\n",
      "[2 5 2 6 9 8 8 4 6 3] == [2 5 2 6 9 8 8 4 6 3]\n",
      "[0 1 5 4 2 4 8 6 7 7] == [0 1 5 4 2 4 8 6 7 7]\n",
      "[7 2 8 0 7 4 6 8 7 2] == [7 2 8 0 7 4 6 8 7 2]\n",
      "[7 8 1 0 9 5 8 5 8 7] == [7 8 1 0 9 5 8 5 8 7]\n",
      "[0 3 5 3 6 2 5 0 1 8] == [0 3 5 3 6 2 5 0 1 8]\n",
      "[4 7 3 5 8 9 9 6 3 7] == [4 7 3 5 8 9 9 6 3 7]\n",
      "[4 5 3 1 1 0 2 5 7 9] == [4 5 3 1 1 0 2 5 7 9]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Load data\n",
    "data = np.load('example/example.npz')\n",
    "_x, _y = data[\"_x\"], data[\"_y\"]\n",
    "_x = np.concatenate((_x, np.expand_dims(_y, axis=1)), 1)\n",
    "\n",
    "# Write to a csv file\n",
    "_x_str = np.array_str(_x)\n",
    "_x_str = re.sub(\"[\\[\\]]\", \"\", _x_str)\n",
    "_x_str = re.sub(\"(?m)^ +\", \"\", _x_str)\n",
    "_x_str = re.sub(\"[ ]+\", \",\", _x_str)\n",
    "with open('example/example.csv', 'w') as fout:\n",
    "    fout.write(_x_str)\n",
    "    \n",
    "# Hyperparams\n",
    "batch_size = 10\n",
    "\n",
    "# Create a string queue\n",
    "fname_q = tf.train.string_input_producer([\"example/example.csv\"])\n",
    "\n",
    "# Q8. Create a TextLineReader\n",
    "reader = tf.TextLineReader()\n",
    "\n",
    "# Read the string queue\n",
    "_, value = reader.read(fname_q)\n",
    "\n",
    "# Q9. Decode value\n",
    "record_defaults = [[0]]*10\n",
    "col1, col2, col3, col4, col5, col6, col7, col8, col9, col10 = tf.decode_csv(\n",
    "    value,\n",
    "    record_defaults=record_defaults)\n",
    "x = tf.stack([col1, col2, col3, col4, col5, col6, col7, col8, col9])\n",
    "y = col10\n",
    "\n",
    "# Batching\n",
    "x_batch, y_batch = tf.train.shuffle_batch(\n",
    "      [x, y], batch_size=batch_size, capacity=200, min_after_dequeue=100)\n",
    "\n",
    "# Ops\n",
    "y_hat = 45 - tf.reduce_sum(x_batch, axis=1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    for i in range(num_epochs*10):\n",
    "        _y_hat, _y_batch = sess.run([y_hat, y_batch])\n",
    "        print(_y_hat, \"==\", _y_batch)\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Read image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 samples have been seen\n",
      "20 samples have been seen\n",
      "30 samples have been seen\n",
      "40 samples have been seen\n",
      "50 samples have been seen\n",
      "60 samples have been seen\n",
      "70 samples have been seen\n",
      "80 samples have been seen\n",
      "90 samples have been seen\n",
      "100 samples have been seen\n",
      "Done training -- epoch limit reached\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Hyperparams\n",
    "batch_size = 10\n",
    "num_epochs = 1\n",
    "\n",
    "# Make fake images and save\n",
    "for i in range(100):\n",
    "    _x = np.random.randint(0, 256, size=(10, 10, 4)).astype(np.uint8)\n",
    "    plt.imsave(\"example/image_{}.jpg\".format(i), _x)\n",
    "\n",
    "# Import jpg files\n",
    "images = tf.train.match_filenames_once('example/*.jpg')\n",
    "\n",
    "# Create a string queue\n",
    "fname_q = tf.train.string_input_producer(images, num_epochs=num_epochs, shuffle=True)\n",
    "\n",
    "# Q10. Create a WholeFileReader\n",
    "reader = tf.WholeFileReader()\n",
    "\n",
    "# Read the string queue\n",
    "_, value = reader.read(fname_q)\n",
    "\n",
    "# Q11. Decode value\n",
    "img = tf.image.decode_image(value)\n",
    "\n",
    "# Batching\n",
    "img_batch = tf.train.batch([img], shapes=([10, 10, 4]), batch_size=batch_size)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\n",
    "\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    num_samples = 0\n",
    "    try:\n",
    "        while not coord.should_stop():\n",
    "            sess.run(img_batch)\n",
    "            num_samples += batch_size\n",
    "            print(num_samples, \"samples have been seen\")\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done training -- epoch limit reached')\n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Anaconda 3",
   "language": "python",
   "name": "anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
